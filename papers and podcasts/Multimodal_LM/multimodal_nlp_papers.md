# 🧠 Foundational NLP Research Papers

A categorized and chronologically ordered list of seminal research papers in Natural Language Processing (NLP).

---

## 🤝 Multimodal Learning (Vision + Text + Audio)

### 🖼️ Vision + Language
- **2015**: *Show and Tell* – Vinyals et al.  
  _CNN + LSTM architecture for image captioning._
- **2016**: *Show, Attend and Tell* – Xu et al.  
  _Introduced attention mechanism into image captioning._
- **2021**: *CLIP* – Radford et al. (OpenAI)  
  _Contrastive pretraining of image and text encoders with large-scale noisy web data._
- **2021**: *ALIGN* – Jia et al. (Google)  
  _Scaled-up CLIP-style training with noisy alt-text from web images._
- **2022**: *BLIP* – Li et al.  
  _Bootstrapped language-image pretraining using captioning and vision-language matching._
- **2023**: *Segment Anything (SAM)* – Kirillov et al.  
  _Foundation model for zero-shot segmentation using promptable inputs._

### 🔉 Audio + Text (Speech Models)
- **2016**: *Listen, Attend and Spell* – Chan et al.  
  _End-to-end speech recognition with attention-based seq2seq._
- **2017**: *Tacotron* – Wang et al.  
  _End-to-end text-to-speech synthesis using spectrogram prediction._
- **2020**: *wav2vec 2.0* – Baevski et al.  
  _Self-supervised learning of speech representations for ASR._
- **2023**: *Whisper* – OpenAI  
  _Multilingual and multitask speech recognition and transcription._

### 🎥 Multimodal Reasoning (Vision + Audio + Language)
- **2021**: *MERLOT* – Zellers et al.  
  _Learning commonsense from videos, subtitles, and images._
- **2022**: *Flamingo* – DeepMind  
  _Few-shot visual language model for images and video understanding._
- **2023**: *GPT-4V* – OpenAI  
  _Visual extension of GPT-4, capable of vision-language reasoning._
- **2023**: *AudioPaLM* – Google DeepMind  
  _Joint speech-text foundation model combining AudioLM and PaLM._
- **2023**: *MM1* – Meta  
  _Multimodal multitask pretraining across image, audio, and text._

### 🧪 Cross-modal Generation
- **2022**: *Make-A-Video* – Meta  
  _Text-to-video generation using diffusion-based models._
- **2023**: *MusicLM* – Google  
  _Text-to-music generation model leveraging audio hierarchies._
- **2023**: *VideoPoet* – Google  
  _Autoregressive transformer for text, audio, and video generation._

