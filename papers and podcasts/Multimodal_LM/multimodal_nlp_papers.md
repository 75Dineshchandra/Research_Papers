# ğŸ§  Foundational NLP Research Papers

A categorized and chronologically ordered list of seminal research papers in Natural Language Processing (NLP).

---

## ğŸ¤ Multimodal Learning (Vision + Text + Audio)

### ğŸ–¼ï¸ Vision + Language
- **2015**: *Show and Tell* â€“ Vinyals et al.  
  _CNN + LSTM architecture for image captioning._
- **2016**: *Show, Attend and Tell* â€“ Xu et al.  
  _Introduced attention mechanism into image captioning._
- **2021**: *CLIP* â€“ Radford et al. (OpenAI)  
  _Contrastive pretraining of image and text encoders with large-scale noisy web data._
- **2021**: *ALIGN* â€“ Jia et al. (Google)  
  _Scaled-up CLIP-style training with noisy alt-text from web images._
- **2022**: *BLIP* â€“ Li et al.  
  _Bootstrapped language-image pretraining using captioning and vision-language matching._
- **2023**: *Segment Anything (SAM)* â€“ Kirillov et al.  
  _Foundation model for zero-shot segmentation using promptable inputs._

### ğŸ”‰ Audio + Text (Speech Models)
- **2016**: *Listen, Attend and Spell* â€“ Chan et al.  
  _End-to-end speech recognition with attention-based seq2seq._
- **2017**: *Tacotron* â€“ Wang et al.  
  _End-to-end text-to-speech synthesis using spectrogram prediction._
- **2020**: *wav2vec 2.0* â€“ Baevski et al.  
  _Self-supervised learning of speech representations for ASR._
- **2023**: *Whisper* â€“ OpenAI  
  _Multilingual and multitask speech recognition and transcription._

### ğŸ¥ Multimodal Reasoning (Vision + Audio + Language)
- **2021**: *MERLOT* â€“ Zellers et al.  
  _Learning commonsense from videos, subtitles, and images._
- **2022**: *Flamingo* â€“ DeepMind  
  _Few-shot visual language model for images and video understanding._
- **2023**: *GPT-4V* â€“ OpenAI  
  _Visual extension of GPT-4, capable of vision-language reasoning._
- **2023**: *AudioPaLM* â€“ Google DeepMind  
  _Joint speech-text foundation model combining AudioLM and PaLM._
- **2023**: *MM1* â€“ Meta  
  _Multimodal multitask pretraining across image, audio, and text._

### ğŸ§ª Cross-modal Generation
- **2022**: *Make-A-Video* â€“ Meta  
  _Text-to-video generation using diffusion-based models._
- **2023**: *MusicLM* â€“ Google  
  _Text-to-music generation model leveraging audio hierarchies._
- **2023**: *VideoPoet* â€“ Google  
  _Autoregressive transformer for text, audio, and video generation._

