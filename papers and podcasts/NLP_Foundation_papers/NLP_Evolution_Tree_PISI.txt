https://chatgpt.com/share/6873c436-69c4-800a-bf50-235b14fd11d5

/NLP_Evolution
â”œâ”€â”€ Text_Preprocessing_&_Representation
â”‚   â”œâ”€â”€ Subword_Units (BPE â€“ Sennrich et al., 2015)
â”‚   â”‚   â”œâ”€â”€ Problem: OOV words, fixed vocabulary, productive morphology
â”‚   â”‚   â”œâ”€â”€ Innovation: Subword units (BPE, char n-grams) for NMT
â”‚   â”‚   â”œâ”€â”€ Solution: Merge frequent character pairs until vocabulary budget met
â”‚   â”‚   â””â”€â”€ Impact: Standard tokenization for Transformer models
â”‚   â””â”€â”€ Word_Embeddings
â”‚       â”œâ”€â”€ Word2Vec (Mikolov et al., 2013)
â”‚       â”‚   â”œâ”€â”€ Problem: One-hot vectors lack semantic similarity
â”‚       â”‚   â”œâ”€â”€ Innovation: Predictive learning of word context
â”‚       â”‚   â”œâ”€â”€ Solution: CBOW & Skip-gram architectures
â”‚       â”‚   â””â”€â”€ Impact: Dense embeddings enable semantic arithmetic
â”‚       â”œâ”€â”€ GloVe (Pennington et al., 2014)
â”‚       â”‚   â”œâ”€â”€ Problem: Local context misses global co-occurrence
â”‚       â”‚   â”œâ”€â”€ Innovation: Train on word-word co-occurrence matrix
â”‚       â”‚   â”œâ”€â”€ Solution: Global log-bilinear regression model
â”‚       â”‚   â””â”€â”€ Impact: High-quality, interpretable embeddings
â”‚       â””â”€â”€ ELMo (Peters et al., 2018)
â”‚           â”œâ”€â”€ Problem: Static embeddings ignore polysemy
â”‚           â”œâ”€â”€ Innovation: Deep bidirectional LSTMs with character input
â”‚           â”œâ”€â”€ Solution: Contextual word representations
â”‚           â””â”€â”€ Impact: Major leap in contextual NLP models
â”œâ”€â”€ Transformers_&_Pre-trained_Language_Models
â”‚   â”œâ”€â”€ Transformer (Vaswani et al., 2017)
â”‚   â”‚   â”œâ”€â”€ Problem: RNNs are sequential and slow
â”‚   â”‚   â”œâ”€â”€ Innovation: Attention-only architecture
â”‚   â”‚   â”œâ”€â”€ Solution: Self-attention + multi-head attention + positional encodings
â”‚   â”‚   â””â”€â”€ Impact: Breakthrough in training efficiency and scalability
â”‚   â”œâ”€â”€ GPT-1 (Radford et al., 2018)
â”‚   â”‚   â”œâ”€â”€ Problem: Lack of general pre-trained models
â”‚   â”‚   â”œâ”€â”€ Innovation: Generative pretraining + task-specific fine-tuning
â”‚   â”‚   â”œâ”€â”€ Solution: Transformer decoder trained on next-token prediction
â”‚   â”‚   â””â”€â”€ Impact: Start of pretrain-then-finetune paradigm
â”‚   â”œâ”€â”€ BERT (Devlin et al., 2018)
â”‚   â”‚   â”œâ”€â”€ Problem: One-directional context in GPT-1
â”‚   â”‚   â”œâ”€â”€ Innovation: Bidirectional masked language modeling
â”‚   â”‚   â”œâ”€â”€ Solution: Mask words and predict from both sides
â”‚   â”‚   â””â”€â”€ Impact: Set state-of-the-art on NLU benchmarks
â”‚   â””â”€â”€ T5 (Raffel et al., 2020)
â”‚       â”œâ”€â”€ Problem: Diverse tasks had fragmented formats
â”‚       â”œâ”€â”€ Innovation: Unified text-to-text framework
â”‚       â”œâ”€â”€ Solution: Encode everything as a text generation problem
â”‚       â””â”€â”€ Impact: Strong general-purpose model
â”œâ”€â”€ ğŸ”„ LLM_Scaling
â”‚   â”œâ”€â”€ GPT-2 (2019)
â”‚   â”‚   â”œâ”€â”€ Problem: Unclear if scale alone yields generalization
â”‚   â”‚   â”œâ”€â”€ Innovation: Larger LM trained on WebText corpus
â”‚   â”‚   â”œâ”€â”€ Solution: Train 1.5B parameter model on next token prediction
â”‚   â”‚   â””â”€â”€ Impact: Emergent capabilities, few-shot abilities appear
â”‚   â”œâ”€â”€ GPT-3 (2020)
â”‚   â”‚   â”œâ”€â”€ Problem: Fine-tuning per task is costly
â”‚   â”‚   â”œâ”€â”€ Innovation: Prompt-based few-shot learning
â”‚   â”‚   â”œâ”€â”€ Solution: Train 175B model to learn from examples in prompts
â”‚   â”‚   â””â”€â”€ Impact: Prompt engineering era begins, general LLM APIs emerge
â”‚   â””â”€â”€ InstructGPT (2022)
â”‚       â”œâ”€â”€ Problem: GPT-3 not aligned to helpful responses
â”‚       â”œâ”€â”€ Innovation: Reinforcement Learning from Human Feedback (RLHF)
â”‚       â”œâ”€â”€ Solution: Rank outputs â†’ train reward model â†’ PPO fine-tuning
â”‚       â””â”€â”€ Impact: Foundation for ChatGPT, improved alignment
â”œâ”€â”€ âš™ï¸ Retrieval-Augmented_Generation
â”‚   â”œâ”€â”€ REALM (2020)
â”‚   â”‚   â”œâ”€â”€ Problem: LLMs memorize facts â†’ outdated knowledge
â”‚   â”‚   â”œâ”€â”€ Innovation: End-to-end differentiable retrieval
â”‚   â”‚   â”œâ”€â”€ Solution: Learn to retrieve Wikipedia passages during training
â”‚   â”‚   â””â”€â”€ Impact: Foundation for retriever-reader pipelines
â”‚   â”œâ”€â”€ RAG (2020)
â”‚   â”‚   â”œâ”€â”€ Problem: REALM too tightly integrated
â”‚   â”‚   â”œâ”€â”€ Innovation: Modular retriever + generator
â”‚   â”‚   â”œâ”€â”€ Solution: DPR + BART = retrieve docs then generate answers
â”‚   â”‚   â””â”€â”€ Impact: Core architecture for RAG pipelines today
â”‚   â””â”€â”€ FiD (2021)
â”‚       â”œâ”€â”€ Problem: Concatenating docs confuses LLM
â”‚       â”œâ”€â”€ Innovation: Encode docs separately, fuse in decoder
â”‚       â”œâ”€â”€ Solution: T5 encodes passages independently, decoder fuses
â”‚       â””â”€â”€ Impact: Strongest model for open-domain QA
